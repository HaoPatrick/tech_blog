<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"  lang="en-us" >

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" /> 
    <title>Paper Review: Accelerators | Xiangpeng Hao</title>
     <meta property='og:title' content='Paper Review: Accelerators - Xiangpeng Hao'>
<meta property='og:description' content='Accelerators, especially the FPGAs, have gained increasingly more popularity both in the industry and academia. The major reason behind this popularity stems from the end of Moore&rsquo;s law, where people can not expect the CPU performance to grow exponentially to solve existing performance issues. In other words, we need to take actions to improve the system performance, otherwise it will likely to remain unchanged for the next few years.'>
<meta property='og:url' content='/posts/paper-review-6/'>
<meta property='og:site_name' content='Xiangpeng Hao'>
<meta property='og:type' content='article'><meta property='article:section' content='Posts'><meta property='article:published_time' content='2020-03-01T18:17:29-08:00'/><meta property='article:modified_time' content='2020-03-01T18:17:29-08:00'/><meta name='twitter:card' content='summary'><meta name='twitter:site' content='@'><meta name='twitter:creator' content='@'>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="shortcut icon" href="favicon.png" /></head>

<body>
<section class="section">
  <div class="container">
    <nav class="nav">
      <div class="nav-left">
        <a class="nav-item" href="/"><h1 class="title is-4">Xiangpeng Hao</h1></a>
      </div>
      <div class="nav-right">
        <nav class="nav-item level is-mobile"><a class="level-item" href='https://haoxp.xyz' target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <g transform="scale(0.042)" stroke-width="4">
        <path fill="currentColor"
            d="M528 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h480c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zm0 400H48V80h480v352zM208 256c35.3 0 64-28.7 64-64s-28.7-64-64-64-64 28.7-64 64 28.7 64 64 64zm-89.6 128h179.2c12.4 0 22.4-8.6 22.4-19.2v-19.2c0-31.8-30.1-57.6-67.2-57.6-10.8 0-18.7 8-44.8 8-26.9 0-33.4-8-44.8-8-37.1 0-67.2 25.8-67.2 57.6v19.2c0 10.6 10 19.2 22.4 19.2zM360 320h112c4.4 0 8-3.6 8-8v-16c0-4.4-3.6-8-8-8H360c-4.4 0-8 3.6-8 8v16c0 4.4 3.6 8 8 8zm0-64h112c4.4 0 8-3.6 8-8v-16c0-4.4-3.6-8-8-8H360c-4.4 0-8 3.6-8 8v16c0 4.4 3.6 8 8 8zm0-64h112c4.4 0 8-3.6 8-8v-16c0-4.4-3.6-8-8-8H360c-4.4 0-8 3.6-8 8v16c0 4.4 3.6 8 8 8z">
        </path>
        
    </g>
    
</svg></i>
            </span>
          </a><a class="level-item" href='https://github.com/XiangpengHao' target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path
        d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22" />
    
</svg></i>
            </span>
          </a><a class="level-item" href='https://linkedin.com/in/hao-xiangpeng' target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path stroke-width="1.8"
        d="m5.839218,4.101561c0,1.211972 -0.974141,2.194011 -2.176459,2.194011s-2.176459,-0.982039 -2.176459,-2.194011c0,-1.211094 0.974141,-2.194011 2.176459,-2.194011s2.176459,0.982917 2.176459,2.194011zm0.017552,3.94922l-4.388022,0l0,14.04167l4.388022,0l0,-14.04167zm7.005038,0l-4.359939,0l0,14.04167l4.360816,0l0,-7.370999c0,-4.098413 5.291077,-4.433657 5.291077,0l0,7.370999l4.377491,0l0,-8.89101c0,-6.915523 -7.829986,-6.66365 -9.669445,-3.259423l0,-1.891237z" />
    
</svg></i>
            </span>
          </a><a class="level-item" href='https://blog.haoxp.xyz/index.xml' target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <g transform="scale(0.045)" stroke-width="40">
        <path
            d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z">
        </path>
    </g>
    
    
</svg></i>
            </span>
          </a><a class="level-item" href='https://t.me/newsathlh' target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path
        d="m 22.05,1.577 c -0.393,-0.016 -0.784,0.08 -1.117,0.235 -0.484,0.186 -4.92,1.902 -9.41,3.64 C 9.263,6.325 7.005,7.198 5.267,7.867 3.53,8.537 2.222,9.035 2.153,9.059 c -0.46,0.16 -1.082,0.362 -1.61,0.984 -0.79581202,1.058365 0.21077405,1.964825 1.004,2.499 1.76,0.564 3.58,1.102 5.087,1.608 0.556,1.96 1.09,3.927 1.618,5.89 0.174,0.394 0.553,0.54 0.944,0.544 l -0.002,0.02 c 0,0 0.307,0.03 0.606,-0.042 0.3,-0.07 0.677,-0.244 1.02,-0.565 0.377,-0.354 1.4,-1.36 1.98,-1.928 l 4.37,3.226 0.035,0.02 c 0,0 0.484,0.34 1.192,0.388 0.354,0.024 0.82,-0.044 1.22,-0.337 0.403,-0.294 0.67,-0.767 0.795,-1.307 0.374,-1.63 2.853,-13.427 3.276,-15.38 L 23.676,4.725 C 23.972,3.625 23.863,2.617 23.18,2.02 22.838,1.723 22.444,1.593 22.05,1.576 Z" />
    
</svg></i>
            </span>
          </a></nav>
      </div>
    </nav>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="subtitle is-6 is-pulled-right">
      
    </div>
    <h2 class="subtitle is-6">March 1, 2020</h2>
    <h1 class="title">Paper Review: Accelerators</h1>
    
    <div class="content">
      

<p>Accelerators, especially the FPGAs, have gained increasingly more popularity both in the industry and academia.
The major reason behind this popularity stems from the end of Moore&rsquo;s law, where people can not expect the CPU performance to grow exponentially to solve existing performance issues.
In other words, we need to take actions to improve the system performance, otherwise it will likely to remain unchanged for the next few years.</p>

<p>FPGAs are sound and well-established co-processor in improving the performance of overall system.
FPGAs, however, are not the answer to all performance problems. Rather, they&rsquo;re just some high performance co-processors that can speed up some computation <strong>at some cost</strong>.
It&rsquo;s <strong>a research problem</strong> in determining in which role the FPGAs can maximize their potentials,
and most importantly, what is the computing pattern for future high-performance systems?</p>

<p>The first paper talks about near-data processing.
The main motivation is to reduce data movement between the clients and the servers.
FPGAs act as embedded yet feature-complete computation power on edge devices.
The second paper tries to integrate the FPGAs into an existing computation intensive data processing pipeline.
In contrast to the first paper where the authors try to use FPGAs on data-intensive system,
in the second paper the authors uses FPGAs to address computation-intensive problems.
Combining these two paper is more interesting than looking them separately.</p>

<h3 id="caribou-intelligent-distributed-storage">Caribou: Intelligent Distributed Storage</h3>

<p>The most thought provoking word in this paper is <strong>near-data processing</strong>.
For the past few decades our computation model has been centered around a central processing unit, a.k.a CPU.</p>

<p>We were trained to think in this way: to compute any thing, we need to move data from storage to memory,
then to the CPU cache; after the computation, we flush the CPU cache back to the memory and eventually persist the data on the storage.
We were fine with this paradigm because our data were relatively small,
our data-caching policy were extremely effective and our parallel computation power was limited.
But the recent trends force us out of the comfort zone: the number of data grows exponentially,
the number of threads grows exponentially, while the single thread performance is believed to be stable.</p>

<p>Moving data across the indirection hierarchy has been one of the major performance bottlenecks.
So people think about ways to address this problem, and using FPGA is one of the potential answers.</p>

<p>This paper presents Caribou, a FPGA-implemented key value store.
The concept of near-data processing comes from FPGA processing on the embedded DRAM.</p>

<p>(more on the presentation)</p>

<h3 id="lowering-the-latency-of-data-processing-pipelines-through-fpga-based-hardware-acceleration">Lowering the Latency of Data Processing Pipelines Through FPGA based Hardware Acceleration</h3>

<p>The authors try to replace a CPU-based machine learning model to a FGPA-based model.
The machine learning inference is known to be floating-number intensive and is largely limited by the CPU clock frequency.
A FPGA chip can definitely help under such workload, not only does it have better throughput, but also be an order of more power efficient.</p>

<p>The cost of FPGA in terms of throughput, however, is data movement (which is exactly the second paper trying to address).
Data interaction between the PCIe (and even the network) can be costly compared to that with the main memory.
Given what we have discussed, the workload need to be sufficiently heavy in order to accommodate the data movement cost.
Machine learning inferences is a good fit, while other workloads (e.g. graph processing or single index query) might not be.</p>

<p>The majority parts of the paper is boring because it looks like just a new system that utilized the FPGA, but there are several sparkling points:</p>

<ol>
<li>Why not GPUs? When talking about accelerator, the GPUs are probably the most widely researched and deployed ones.
This paper has a tiny section that answers this question, and there&rsquo;s a particularly important sentence: &ldquo;we are not aware of any significance on inference over decision tree ensembles on GPUs due to the irregular nature of the computations.&rdquo;
The answer is simple: GPU is good at handling regular data while that particular machine learning model happens to be dominated by irregular memory access.</li>
<li>What are the data movement cost? The authors installed the FPGAs on PCIe channels and fast network.
The evaluations (Figure 9.c) shows the CPU-FPGA bandwidth indeed limits the scalability of the whole system.
Nevertheless, the benefits of FPGS acceleration is so significant that the data movement in the target scenario is negligible.</li>
</ol>

      
    </div>
    
  </div>
</section>


<section class="section">
  <div class="container has-text-centered">
    <p><a href="https://www.haoxp.xyz">Xiangpeng Hao</a> 2020</p>
    
  </div>
</section>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-66103952-7', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


</body>
</html>

